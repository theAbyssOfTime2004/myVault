2025-07-06 01:10


Tags:

# Attention Mechanism

## Pipeline cá»¥ thá»ƒ cá»§a **Seq2Seq + Attention vá»›i encoder BiGRU vÃ  decoder GRU**
---
### 1. Input Preprocessing

- Input sentence (e.g., French) â†’ tokenization
- Convert tokens â†’ word embeddings:  
  $x_1, x_2, ..., x_T$

### 2. Encoder (BiGRU)

- Má»—i input token Ä‘Æ°á»£c Ä‘Æ°a vÃ o má»™t BiGRU:
  - GRU Ä‘á»c xuÃ´i â†’ $\overrightarrow{h}_t$
  - GRU Ä‘á»c ngÆ°á»£c â†’ $\overleftarrow{h}_t$
- GhÃ©p láº¡i:
  - $h_t^{enc} = [\overrightarrow{h}_t ; \overleftarrow{h}_t]$
- DÃ£y encoder hidden states:
  - $H^{enc} = \{h_1^{enc}, h_2^{enc}, ..., h_T^{enc}\}$

### 3. Khá»Ÿi táº¡o Decoder

- Láº¥y hidden state cuá»‘i cÃ¹ng cá»§a backward encoder:
  - $h_0^{dec} = \overleftarrow{h}_1$
- (CÃ³ thá»ƒ Ä‘Æ°a qua linear layer náº¿u cáº§n)

### 4. Decoder - bÆ°á»›c Ä‘áº§u tiÃªn

- Input: embedding cá»§a token `<START>` â†’ $x_0$
- (TÃ¹y chá»n): cá»™ng thÃªm context vector $c_0$ náº¿u dÃ¹ng á»Ÿ bÆ°á»›c Ä‘áº§u

- TÃ­nh hidden state Ä‘áº§u tiÃªn:
  - $h_1^{dec} = \text{GRU}([x_0 ; c_0], h_0^{dec})$

### 5. Attention (má»—i bÆ°á»›c $t$)

1. TÃ­nh attention score giá»¯a $h_t^{dec}$ vÃ  tá»«ng $h_i^{enc}$:  
   - $\text{score}_i = v^\top \tanh(W_1 h_t^{dec} + W_2 h_i^{enc})$ (*concat*)

2. DÃ¹ng softmax Ä‘á»ƒ chuáº©n hÃ³a:  
   - $\alpha_i = \text{softmax}(\text{score}_i)$

1. TÃ­nh context vector (weighted sum):  
   - $c_t = \sum_i \alpha_i \cdot h_i^{enc}$

### 6. Dá»± Ä‘oÃ¡n tá»« Ä‘áº§u ra

- GhÃ©p $h_t^{dec}$ vÃ  $c_t$:  
  - $o_t = [h_t^{dec} ; c_t]$
- Dá»± Ä‘oÃ¡n tá»«:
  - $\hat{y}_t = \text{softmax}(W_o o_t + b_o)$

- Náº¿u **training**:
  - DÃ¹ng ground-truth tá»« trÆ°á»›c Ä‘Ã³ (teacher forcing)
- Náº¿u **inference**:
  - Láº¥y tá»« cÃ³ xÃ¡c suáº¥t cao nháº¥t â†’ dÃ¹ng lÃ m input tiáº¿p theo

### 7. Dá»«ng khi

- Sinh ra token `<END>`, hoáº·c  
- Äáº¡t Ä‘áº¿n Ä‘á»™ dÃ i tá»‘i Ä‘a
---

## Tá»•ng quan kiáº¿n trÃºc vÃ  pipeline cá»¥ thá»ƒ cá»§a **seq2seq with 2-layer stacked encoder + attention**

| ThÃ nh pháº§n       | MÃ´ táº£                                                                 |
|------------------|----------------------------------------------------------------------|
| **Encoder**      | 2-layer stacked LSTM                                                 |
| **Decoder**      | 2-layer stacked LSTM, khá»Ÿi táº¡o tá»« final hidden + cell states cá»§a encoder |
| **Attention**    | TÃ­nh giá»¯a decoder hidden state hiá»‡n táº¡i $h_t^{dec}$ vÃ  toÃ n bá»™ encoder hidden states $h_i^{enc}$ |
| **Output step**  | GhÃ©p context vector $c_t$ + decoder output $h_t^{dec}$ â†’ Ä‘Æ°a qua feed-forward layer Ä‘á»ƒ sinh tá»« |

### 1. Encoder: 2-layer stacked LSTM

- Má»—i tá»« Ä‘áº§u vÃ o $x_t$ Ä‘Æ°á»£c xá»­ lÃ½ qua **2 táº§ng LSTM liÃªn tiáº¿p**:

  - LSTM layer 1: $x_t \rightarrow h_t^{(1)}$
  - LSTM layer 2: $h_t^{(1)} \rightarrow h_t^{(2)}$

- Táº§ng trÃªn nháº­n hidden state tá»« táº§ng dÆ°á»›i táº¡i cÃ¹ng timestep.
- Táº§ng cuá»‘i cá»§a encoder tráº£ vá»:
  - Hidden state cuá»‘i cÃ¹ng: $h_T^{(2)}$
  - Cell state cuá»‘i cÃ¹ng: $c_T^{(2)}$

â¡ï¸ Sá»­ dá»¥ng $h_T^{(2)}$ vÃ  $c_T^{(2)}$ Ä‘á»ƒ **khá»Ÿi táº¡o decoder**.


### 2. Decoder: 2-layer stacked LSTM

- CÅ©ng gá»“m **2 táº§ng nhÆ° encoder**.
- Khá»Ÿi táº¡o tá»«:
  - Hidden states: $h_0^{dec} = h_T^{(2)}$
  - Cell states: $c_0^{dec} = c_T^{(2)}$

- Táº¡i má»—i timestep $t$:
  - Nháº­n embedding tá»« tá»« trÆ°á»›c: $x_t$
  - TÃ­nh hidden state: $h_t^{dec} = \text{LSTM}(x_t, h_{t-1}^{dec}, c_{t-1}^{dec})$
  - TÃ­nh context vector $c_t$ tá»« attention

### 3. Attention Score Functions Ä‘Æ°á»£c dÃ¹ng

### Má»¥c tiÃªu:
TÃ­nh Ä‘iá»ƒm attention score giá»¯a $h_t^{dec}$ vÃ  tá»«ng $h_i^{enc}$ Ä‘á»ƒ biáº¿t nÃªn táº­p trung vÃ o pháº§n nÃ o cá»§a Ä‘áº§u vÃ o.

| TÃªn hÃ m           | CÃ´ng thá»©c                                                        | Ghi chÃº              |
|-------------------|------------------------------------------------------------------|----------------------|
| **Additive**      | $\text{score}_i = v^\top \tanh(W_1 h_t^{dec} + W_2 h_i^{enc})$   | Bahdanau (concat)    |
| **Dot Product**   | $\text{score}_i = h_t^{dec} \cdot h_i^{enc}$                     | Luong                |
| **General**       | $\text{score}_i = h_t^{dec^\top} W h_i^{enc}$                    | Luong (general)      |
| **Location-based**| $\text{score}_i = W h_t^{dec}$ (khÃ´ng dÃ¹ng $h_i^{enc}$)         | Dá»±a trÃªn vá»‹ trÃ­      |

Sau Ä‘Ã³:

- Chuáº©n hÃ³a scores báº±ng softmax:
  - $\alpha_i = \text{softmax}(\text{score}_i)$
- TÃ­nh context vector:
  - $c_t = \sum_i \alpha_i \cdot h_i^{enc}$


### 4. Sinh tá»« tiáº¿p theo

- GhÃ©p context vector vÃ  decoder hidden state:
  - $o_t = [h_t^{dec} ; c_t]$
- ÄÆ°a qua feedforward layer + softmax:
  - $\hat{y}_t = \text{softmax}(W_o o_t + b_o)$

- Náº¿u Ä‘ang **training**:
  - Sá»­ dá»¥ng ground truth $y_{t-1}$ lÃ m input tiáº¿p theo (teacher forcing)

- Náº¿u Ä‘ang **inference**:
  - Láº¥y tá»« cÃ³ xÃ¡c suáº¥t cao nháº¥t tá»« $\hat{y}_t$ lÃ m input tiáº¿p theo


### Ghi chÃº

- $h_t^{dec}$: decoder hidden state táº¡i thá»i Ä‘iá»ƒm $t$
- $h_i^{enc}$: encoder hidden state táº¡i thá»i Ä‘iá»ƒm $i$
- $c_t$: context vector táº¡i thá»i Ä‘iá»ƒm $t$
- $\alpha_i$: attention weight táº¡i vá»‹ trÃ­ $i$
- $o_t$: vector tá»•ng há»£p tá»« decoder + attention
- $\hat{y}_t$: xÃ¡c suáº¥t phÃ¢n phá»‘i tá»« vá»±ng á»Ÿ bÆ°á»›c $t$

---
## Image Caption Generation with Attention

### Tá»•ng quan

**Image Captioning** lÃ  bÃ i toÃ¡n:
> Input: má»™t hÃ¬nh áº£nh  
> Output: má»™t chuá»—i tá»« mÃ´ táº£ ná»™i dung áº£nh

VÃ­ dá»¥:
-  HÃ¬nh áº£nh: con chim Ä‘ang bay trÃªn máº·t nÆ°á»›c  
- Caption sinh ra: `"A bird flying over a body of water"`

### Pipeline tá»•ng quÃ¡t (Slide 2)

1. **Input Image**: áº£nh Ä‘áº§u vÃ o.
2. **Convolutional Feature Extraction**:
   - DÃ¹ng CNN (VD: Inception, ResNet) Ä‘á»ƒ chia áº£nh thÃ nh nhiá»u vÃ¹ng (regions) â†’ má»—i vÃ¹ng lÃ  1 vector Ä‘áº·c trÆ°ng.
   - VÃ­ dá»¥: áº£nh â†’ `14Ã—14 = 196` vectors (feature map).
3. **RNN with Attention**:
   - DÃ¹ng LSTM Ä‘á»ƒ sinh tá»« tá»«ng bÆ°á»›c.
   - Má»—i bÆ°á»›c, attention sáº½ quyáº¿t Ä‘á»‹nh nÃªn "nhÃ¬n" vÃ¹ng áº£nh nÃ o.
4. **Caption Generation**:
   - Sinh tá»«ng tá»«, vÃ­ dá»¥:
     - `"A"` â†’ `"bird"` â†’ `"flying"` â†’ `"over"` â†’ `"a"` â†’ `"body"` â†’ `"of"` â†’ `"water"`

### CÆ¡ cháº¿ Attention (Slide 1)

Giá»‘ng nhÆ° Attention trong NLP nhÆ°ng thay vÃ¬ tá»« â†’ mÃ´ hÃ¬nh chÃº Ã½ vÃ o **vÃ¹ng áº£nh**.

#### Diá»…n giáº£i:

- Má»—i vÃ¹ng áº£nh sau CNN cÃ³ vector Ä‘áº·c trÆ°ng:  
  `hÂ¹, hÂ², ..., hâ´` (VD: 4 vÃ¹ng áº£nh tÆ°Æ¡ng á»©ng vá»›i 4 kÃ½ tá»± HÃ¡n trong vÃ­ dá»¥ `"æ©Ÿå™¨å­¸ç¿’"`)

- Khi LSTM Ä‘ang sinh tá»« `"learning"`:
  - NÃ³ tÃ­nh attention scores $\alpha^i_1$ giá»¯a $h_t^{dec}$ vÃ  tá»«ng $h^i$
  - Softmax chuáº©n hÃ³a:  
    - $\tilde{\alpha}_1^1 = 0.0$,  
    - $\tilde{\alpha}_1^2 = 0.0$,  
    - $\tilde{\alpha}_1^3 = 0.5$,  
    - $\tilde{\alpha}_1^4 = 0.5$

- TÃ­nh context vector:
  - $c^1 = \sum_i \tilde{\alpha}_1^i h^i = 0.5 h^3 + 0.5 h^4$

- Context vector $c^1$ Ä‘Æ°á»£c Ä‘Æ°a vÃ o RNN Ä‘á»ƒ sinh ra tá»« `"learning"`

---

## ğŸ§  Há»c Ä‘Æ°á»£c "chim lÃ  bird" nhÆ° tháº¿ nÃ o?

> â“ LÃ m sao mÃ´ hÃ¬nh biáº¿t hÃ¬nh con chim thÃ¬ caption nÃªn lÃ  `"bird"`?

- KhÃ´ng Ä‘Æ°á»£c láº­p trÃ¬nh sáºµn!
- MÃ´ hÃ¬nh há»c tá»« **dá»¯ liá»‡u huáº¥n luyá»‡n**:
  - Má»—i áº£nh cÃ³ 5 caption tháº­t (con ngÆ°á»i viáº¿t)
  - Qua huáº¥n luyá»‡n:
    - Khi vector Ä‘áº·c trÆ°ng giá»‘ng chim â†’ mÃ´ hÃ¬nh há»c ráº±ng `"bird"` thÆ°á»ng Ä‘Æ°á»£c sinh ra
    - Attention há»c Ä‘Æ°á»£c vÃ¹ng áº£nh liÃªn quan Ä‘áº¿n tá»« cá»¥ thá»ƒ
- ToÃ n bá»™ Ä‘Æ°á»£c tá»‘i Æ°u qua **backpropagation**, tá»‘i thiá»ƒu hÃ³a loss giá»¯a caption dá»± Ä‘oÃ¡n vÃ  caption tháº­t

---

## ğŸ–¼ï¸ Slide 3 â€“ Attention Ä‘Ãºng chá»—

Má»™t sá»‘ vÃ­ dá»¥ trong bÃ i bÃ¡o "Show, Attend and Tell":

| Caption                              | Attention nhÃ¬n vÃ oâ€¦                      |
|--------------------------------------|------------------------------------------|
| A woman is throwing a frisbee       | tay vÃ  váº­t thá»ƒ bay                       |
| A stop sign on a road               | biá»ƒn bÃ¡o Ä‘á»                              |
| A giraffe standing in a forest      | vÃ¹ng chá»©a hÆ°Æ¡u cao cá»•                    |
| A group of people sitting in a boat | vÃ¹ng cÃ³ ngÆ°á»i vÃ  thuyá»n                  |
| A little girl is blowing a bubble   | máº·t vÃ  bong bÃ³ng                         |

â¡ï¸ Attention giÃºp caption khÃ´ng bá»‹ mÆ¡ há»“ â€” chá»n Ä‘Ãºng vÃ¹ng áº£nh táº¡i Ä‘Ãºng thá»i Ä‘iá»ƒm sinh tá»«.

---

## âœï¸ Tá»•ng káº¿t ká»¹ thuáº­t

| ThÃ nh pháº§n      | Vai trÃ² |
|-----------------|--------|
| **CNN**         | Chia áº£nh thÃ nh nhiá»u vÃ¹ng, má»—i vÃ¹ng thÃ nh vector Ä‘áº·c trÆ°ng |
| **Attention**   | á» má»—i bÆ°á»›c sinh tá»«, tÃ­nh softmax score giá»¯a decoder state vÃ  tá»«ng vÃ¹ng áº£nh |
| **Context Vector** | Tá»•ng trá»ng sá»‘ cÃ¡c vÃ¹ng áº£nh â†’ truyá»n vÃ o RNN |
| **RNN (LSTM)**  | Sinh tá»«ng tá»« dá»±a vÃ o context + tá»« trÆ°á»›c Ä‘Ã³ |
| **Loss**        | So sÃ¡nh caption dá»± Ä‘oÃ¡n vá»›i ground truth â†’ backpropagation |

---

## ğŸ“Œ CÃ´ng thá»©c chÃ­nh

- **Attention Score**:
  - $\text{score}_i = v^\top \tanh(W_1 h_t^{dec} + W_2 h_i^{enc})$
- **Softmax Attention Weights**:
  - $\alpha_i = \text{softmax}(\text{score}_i)$
- **Context Vector**:
  - $c_t = \sum_i \alpha_i \cdot h_i^{enc}$
- **Output Word**:
  - $\hat{y}_t = \text{softmax}(W_o [h_t^{dec}; c_t] + b_o)$

---

## ğŸ§  Ghi nhá»›

> "Attention giÃºp mÃ´ hÃ¬nh khÃ´ng nhÃ¬n toÃ n áº£nh má»™t cÃ¡ch mÃ¹ quÃ¡ng â€” mÃ  chá»n Ä‘Ãºng vÃ¹ng Ä‘á»ƒ sinh Ä‘Ãºng tá»«."

---

## ğŸ“š TÃ i liá»‡u tham kháº£o

**Show, Attend and Tell: Neural Image Caption Generation with Visual Attention**  
Kelvin Xu et al., ICML 2015  
https://arxiv.org/abs/1502.03044


# References
