2025-03-18 14:54


Tags: [[Machine Learning]], [[beginner]]

# Gradient Descent in Linear Regression

### Problems

![[Pasted image 20250318145428.png]]
- Sau khi h·ªçc v·ªÅ [[Linear Regression]], ta bi·∫øt r·∫±ng m√¨nh s·∫Ω ph·∫£i l·∫≠p gi·∫£ thuy·∫øt v·ªÅ h√†m h·ªìi quy v√† t·ª´ ƒë√≥ c√≥ ƒë∆∞·ª£c h√†m loss $J(\theta_{0},\theta_{1})$ v√† ta s·∫Ω mu·ªën minimize loss function $J(\theta_{0},\theta_{1})$ b·∫±ng c√°ch thay ƒë·ªïi c√°c gi√° tr·ªã $\theta_{0},\theta_{1}$
- V·∫•n ƒë·ªÅ l√† l√†m sao ƒë·ªÉ ta c√≥ th·ªÉ thay ƒë·ªïi c√°c gi√° tr·ªã  $\theta_{0},\theta_{1}$ l√†m sao ƒë·ªÉ minimize  $J(\theta_{0},\theta_{1})$ m·ªôt c√°ch h·ª£p l√Ω nh·∫•t
![[Pasted image 20250318150154.png]]
- B·∫Øt ƒë·∫ßu v·ªõi m·ªôt initial value c·ªßa $J(\theta_{0},\theta_{1})$, thu·∫≠t to√°n t·ªëi ∆∞u s·∫Ω d·ªãch chuy·ªÉn gi√° tr·ªã ƒë√≥ theo h∆∞·ªõng m√† s·∫Ω gi√∫p lower the value of  $J(\theta_{0},\theta_{1})$ => **Gradient Descent**
### Gradient Descent

![[Pasted image 20250318150509.png]]

![[Pasted image 20250318150714.png]]
- Kh·ªüi t·∫°o $\theta$
- L·∫∑p l·∫°i thu·∫≠t to√°n ƒë·∫øn khi h·ªôi t·ª•: $$\theta_{j} = \theta_{j} - \alpha \frac{\partial}{\partial \theta_j}J(\theta) $$
- ![[Pasted image 20250318151209.png]]
- H√†m m·∫•t m√°t $J(\theta)$ l√† m·ªôt h√†m l·ªìi (h√¨nh parabol), v√† m·ª•c ti√™u c·ªßa ch√∫ng ta l√† t√¨m gi√° tr·ªã $\theta_j$‚Äã t·ªëi ∆∞u sao cho $J(\theta)$ nh·ªè nh·∫•t. Gradient Descent th·ª±c hi·ªán vi·ªác n√†y b·∫±ng c√°ch **di chuy·ªÉn ng∆∞·ª£c h∆∞·ªõng gradient** ƒë·ªÉ gi·∫£m gi√° tr·ªã $J(\theta)$. Bi·ªÉu th·ª©c ƒë·∫°o h√†m ri√™ng c·ªßa $J(\theta)$ theo bi·∫øn $\theta$:
$$
\frac{\partial J(\theta)}{\partial \theta_j} = \lim_{\epsilon \to 0} \frac{J(\theta \mid \theta_j + \epsilon) - J(\theta \mid \theta_j)}{\epsilon}
$$
- Ta c√≥ 2 tr∆∞·ªùng h·ª£p:
- Gradient d∆∞∆°ng ($\epsilon > 0$)  th√¨ gi·∫£m $\theta_j$
- Gradient √¢m ($\epsilon < 0$) th√¨ tƒÉng $\theta_j$
- Lu√¥n c·∫≠p nh·∫≠t theo h∆∞·ªõng **ng∆∞·ª£c v·ªõi gradient** (c≈©ng c√≥ nghƒ©a l√† ng∆∞·ª£c d·∫•u v·ªõi ƒë·∫°o h√†m) ƒë·ªÉ gi·∫£m h√†m m·∫•t m√°t.
![[Pasted image 20250318152333.png]]
- D·∫•u = ƒë·∫ßu ti√™n, khai tri·ªÉn $J(\theta)$, ta ƒë∆∞·ª£c  $J(\theta) = \frac{1}{2n} \sum_{i=1}^{n} (h_{\theta}(x_{i})-y_{i})^2$
- D·∫•u = th·ª© 2 ti·∫øp t·ª•c khai tri·ªÉn $h_{\theta}(x_{i})$ trong ngo·∫∑c, ta ƒë∆∞·ª£c $\sum_{k=0}^{d} \theta_k x_k^{(i)}$ 
- D·∫•u = th·ª© 3 khai tri·ªÉn ƒë·∫°o h√†m ri√™ng theo bi·∫øn $\theta_j$ cho c·∫£ c·ª•m $J(\theta) = \frac{1}{2n} \sum_{i=1}^{n} (\sum_{k=0}^{d} \theta_k x_k^{(i)}-y_{i})^2$, √°p d·ª•ng quy t·∫Øc ƒë·∫°o h√†m $(u^2)' = 2uu'$ trong ph·∫ßn reminder ta ƒë∆∞·ª£c k·∫øt qu·∫£ nh∆∞ tr√™n, xem $(\sum_{k=0}^{d} \theta_k x_k^{(i)}-y_{i})^2$ nh∆∞ l√† u
- D·∫•u = th·ª© 4  l√† k·∫øt qu·∫£ sau khi r√∫t g·ªçn, ti·∫øp t·ª•c ƒë·∫°o h√†m u, ta ƒë∆∞·ª£c nh∆∞ tr√™n v√¨ $h_{\theta}(x_{i}) = \sum_{k=0}^{d} \theta_k x_k^{(i)}$, n√™n ƒë·∫°o h√†m c·ªßa n√≥ theo $\theta_j$ l√†: $\frac{\partial}{\partial \theta_j} h_{\theta}(x_{i}) = x_j^{(i)}$, v√† ƒë·∫°o h√†m ri√™ng theo $\theta_j$ cho bi·∫øn $y^{(i)}=0$, do ƒë√≥ ta c√≥ ƒë∆∞·ª£c k·∫øt qu·∫£ cu·ªëi c√πng 
![[Pasted image 20250318160249.png]]
- Thu·∫≠t to√°n gradient descent h·ªôi t·ª• khi c√°c ti√™u ch√≠ sau ƒë∆∞·ª£c th·ªèa m√£n:
	- h√†m m·∫•t m√°t $J(\theta)$ ƒë·ªß nh·ªè: $J(\theta)$ b√© h∆°n 1 s·ªë $\epsilon$ c·ª±c nh·ªè th√¨ ta xem r·∫±ng thu·∫≠t to√°n h·ªôi t·ª•
	- kho·∫£ng c√°ch gi·ªØa gi√° tr·ªã $\theta$ m·ªõi v√† c≈© ƒë·ªß nh·ªè: $$ \|Œ∏_{new} - Œ∏_{old}\|_2 < Œµ $$
	- N·∫øu s·ª± thay ƒë·ªïi trong gi√° tr·ªã $\theta$ gi·ªØa 2 l·∫ßn c·∫≠p nh·∫≠t nh·ªè h∆°n 1 s·ªë $\epsilon$ (c·ª±c nh·ªè) th√¨ c√≥ nghƒ©a l√† thu·∫≠t to√°n kh√¥ng c√≤n c·∫£i thi·ªán ƒë√°ng k·ªÉ v√† c√≥ th·ªÉ d·ª´ng
	- ƒê·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh l√† ƒë·ªß t·ªët tr√™n t·∫≠p ki·ªÉm tra
![[Pasted image 20250318212833.png]]
### **1. ƒê·ªì th·ªã b√™n tr√°i: H√†m h·ªìi quy $h(x)=-900-0.1x$**

- Tr·ª•c ho√†nh: **K√≠ch th∆∞·ªõc nh√† (square feet, $x$)**
- Tr·ª•c tung: **Gi√° nh√† (price, $y$)**
- C√°c d·∫•u **x ƒë·ªè**: **D·ªØ li·ªáu hu·∫•n luy·ªán (Training data)**
- ƒê∆∞·ªùng **xanh d∆∞∆°ng**: **ƒê∆∞·ªùng h·ªìi quy hi·ªán t·∫°i** d·ª±a tr√™n tham s·ªë $\theta_0$ v√† $\theta_1$ 
- Ph∆∞∆°ng tr√¨nh hi·ªÉn th·ªã: $h(x)=-900-0.1x$ 
‚Üí ƒê√¢y l√† ph∆∞∆°ng tr√¨nh c·ªßa m√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh hi·ªán t·∫°i.

üìå **√ù nghƒ©a:**
- ƒê·ªì th·ªã n√†y cho th·∫•y m√¥ h√¨nh hi·ªán t·∫°i ch∆∞a ph√π h·ª£p v·ªõi d·ªØ li·ªáu.
- ƒê∆∞·ªùng h·ªìi quy ƒëang c√≥ h·ªá s·ªë g√≥c √¢m ($\theta_1 = -0.1$), c√≥ th·ªÉ ch∆∞a ph·∫£i l√† gi√° tr·ªã t·ªëi ∆∞u.
- C·∫ßn ti·∫øp t·ª•c c·∫≠p nh·∫≠t tham s·ªë ƒë·ªÉ ƒë∆∞·ªùng h·ªìi quy ph√π h·ª£p h∆°n v·ªõi d·ªØ li·ªáu th·ª±c t·∫ø.
### **2. ƒê·ªì th·ªã b√™n ph·∫£i: H√†m m·∫•t m√°t $J(\theta_0,\theta_1)$
- ƒê√¢y l√† **ƒë·ªì th·ªã ƒë∆∞·ªùng ƒë·ªìng m·ª©c (contour map)** th·ªÉ hi·ªán gi√° tr·ªã c·ªßa h√†m m·∫•t m√°t $J(\theta_0,\theta_1)$
- Tr·ª•c ho√†nh: **$\theta_0$ (h·ªá s·ªë ch·∫∑n - bias term)**
- Tr·ª•c tung: **$\theta_1$‚Äã (h·ªá s·ªë g√≥c - slope)**
- C√°c ƒë∆∞·ªùng contour th·ªÉ hi·ªán c√°c m·ª©c gi√° tr·ªã c·ªßa h√†m m·∫•t m√°t $J(\theta_0,\theta_1)$, v·ªõi gi√° tr·ªã c√†ng nh·ªè khi ti·∫øn v·ªÅ t√¢m c·ªßa h√¨nh elip.
- D·∫•u **x ƒë·ªè**: V·ªã tr√≠ hi·ªán t·∫°i c·ªßa c√°c tham s·ªë $(\theta_0,\theta_1)$

üìå **√ù nghƒ©a:**
- M·ª•c ti√™u c·ªßa Gradient Descent l√† t√¨m ƒëi·ªÉm th·∫•p nh·∫•t (global minimum) c·ªßa h√†m m·∫•t m√°t $J(\theta_0,\theta_1)$.
- N·∫øu ƒëi·ªÉm ƒë·ªè ch∆∞a n·∫±m ·ªü trung t√¢m c·ªßa ƒë∆∞·ªùng ƒë·ªìng m·ª©c, nghƒ©a l√† m√¥ h√¨nh ch∆∞a t·ªëi ∆∞u v√† cd·∫ßn ti·∫øp t·ª•c c·∫≠p nh·∫≠t $(\theta_0,\theta_1)$‚Äã b·∫±ng Gradient Descent.
![[Pasted image 20250318212816.png]]
- **M·ª•c ti√™u c·ªßa Gradient Descent**: **T√¨m gi√° tr·ªã t·ªëi ∆∞u c·ªßa $(\theta_0,\theta_1)$ sao cho h√†m m·∫•t m√°t $J(\theta_0,\theta_1)$ nh·ªè nh·∫•t.**  
- **C√°c d·∫•u "√ó" ƒë·ªè th·ªÉ hi·ªán qu√° tr√¨nh c·∫≠p nh·∫≠t tham s·ªë d·∫ßn d·∫ßn v·ªÅ ƒëi·ªÉm t·ªëi ∆∞u.**  
-  **Khi Gradient Descent h·ªôi t·ª•, tham s·ªë $(\theta_0,\theta_1)$ s·∫Ω n·∫±m g·∫ßn trung t√¢m c√°c ƒë∆∞·ªùng ƒë·ªìng m·ª©c.**
-  **Hi·ªÉu ƒë∆°n gi·∫£n**: ƒê·ªì th·ªã n√†y gi·ªëng nh∆∞ m·ªôt b·∫£n ƒë·ªì ƒë·ªãa h√¨nh, trong ƒë√≥ thu·∫≠t to√°n Gradient Descent gi·ªëng nh∆∞ m·ªôt ng∆∞·ªùi leo n√∫i, lu√¥n t√¨m ƒë∆∞·ªùng ƒëi nhanh nh·∫•t xu·ªëng thung l≈©ng n∆°i h√†m m·∫•t m√°t nh·ªè nh·∫•t. 

# References
