- Hiá»‡n táº¡i Ä‘Ã£ cÃ³ baseline model vá»›i kiáº¿n trÃºc gá»“m: 
	- `bert-model`: Ä‘á»ƒ tokenizing vÃ  encoding (táº¡o embeddings)
	- `gnn layer`: gá»“m cÃ³ *sematic graph (dá»±a trÃªn attention)* vÃ  *syntactic graph (sá»­ dá»¥ng dependency tree tá»« scipy*
	- `gate fusion`: Ä‘á»ƒ káº¿t há»£p cáº£ 2 output tá»« *h_syn* vÃ  *h_sem*
	- `mate_classifier`: classifier cho nhiá»‡m vá»¥ MATE - PhÃ¢n loáº¡i má»—i token lÃ  ASPECT hay khÃ´ng
	- `masc_classifier`: classifier cho nhiá»‡m vá»¥ MASC - PhÃ¢n loáº¡i sentiment cho cÃ¡c aspect 
- *Contrastive_loss*: ~~tÃ­nh loss tÆ°Æ¡ng pháº£n giá»¯a aspect vÃ  áº£nh~~
	=> Cáº§n sá»­a láº¡i theo paper - táº­n dá»¥ng *syntactic graph* vÃ  *semantic graph* vá»›i 2 strategy chÃ­nh lÃ  **Cross-scope Contrast** vÃ  **Cross-graph Contrast** 
- cáº§n thay Ä‘á»•i encoder tá»« BERT sang FSUIE-base 
- thá»­ nghiá»‡m pháº§n pretraining gá»“m:
	- táº¡o `SceneGraph`: trong bÃ i bÃ¡o **sá»­ dá»¥ng GPT-4o Ä‘á»ƒ sinh ra cÃ¡c mÃ´ táº£ vÄƒn báº£n cÃ³ cáº¥u trÃºc, chi tiáº¿t vÃ  cá»¥ thá»ƒ hÆ¡n cho má»—i hÃ¬nh áº£nh**
	- dÃ¹ng SceneGraph lÃ m Ä‘áº§u vÃ o cho Qformer (vá»›i tham sá»‘ khá»Ÿi táº¡o láº¥y tá»« 1 bÃ i bÃ¡o khÃ¡c Ä‘Æ°á»£c cited kÃ¨m theo): cho phÃ©p Ã¡nh xáº¡ cÃ¡c img feature -> miá»n vÄƒn báº£n vÃ  thá»±c hiá»‡n viá»‡c káº¿t ná»‘i, cÄƒn chá»‰nh Ä‘áº·c trÆ°ng giá»¯a img vÃ  text. Äiá»u nÃ y giÃºp **giáº£m thiá»ƒu váº¥n Ä‘á» MIM**
	- **Aspect-Oriented enhancement**:cáº£i thiá»‡n Ä‘á»™ nháº¡y trong viá»‡c báº¯t khÃ­a cáº¡nh
	- **Image-Text Matching**
	- **Aspect-level Sentiment-Sensitive Cognition**: tÄƒng kháº£ nÄƒng nháº­n thá»©c sentiment cá»§a mÃ´ hÃ¬nh, trá»±c tiáº¿p **giáº£i quyáº¿t váº¥n Ä‘á» SCP**
	`Loss_p = Loss_ğ‘„ + Loss_ğ´ğ‘‚ğ¸ + Loss_ğ¼ğ‘‡ğ‘€ + Loss_ğ´ğ‘†ğ‘†ğ¶`
	