
"smart_sources:6 - Main Notes/Attention Mechanism.md": {"path":"6 - Main Notes/Attention Mechanism.md","last_embed":{"hash":null},"embeddings":{},"last_read":{"hash":"6m5emd","at":1755153824974},"class_name":"SmartSource","last_import":{"mtime":1751870113544,"size":19406,"at":1755153824991,"hash":"6m5emd"},"blocks":{"#":[1,5],"#Attention Mechanism":[6,492],"#Attention Mechanism#Pipeline cụ thể của **Seq2Seq + Attention với encoder BiGRU và decoder GRU**":[8,68],"#---frontmatter---":[9,67],"#Attention Mechanism#Tổng quan kiến trúc và pipeline cụ thể của **seq2seq with 2-layer stacked encoder + attention**":[69,148],"#Attention Mechanism#Tổng quan kiến trúc và pipeline cụ thể của **seq2seq with 2-layer stacked encoder + attention**#{1}":[71,77],"#Attention Mechanism#Tổng quan kiến trúc và pipeline cụ thể của **seq2seq with 2-layer stacked encoder + attention**#1. Encoder: 2-layer stacked LSTM":[78,92],"#Attention Mechanism#Tổng quan kiến trúc và pipeline cụ thể của **seq2seq with 2-layer stacked encoder + attention**#1. Encoder: 2-layer stacked LSTM#{1}":[80,84],"#Attention Mechanism#Tổng quan kiến trúc và pipeline cụ thể của **seq2seq with 2-layer stacked encoder + attention**#1. Encoder: 2-layer stacked LSTM#{2}":[85,85],"#Attention Mechanism#Tổng quan kiến trúc và pipeline cụ thể của **seq2seq with 2-layer stacked encoder + attention**#1. Encoder: 2-layer stacked LSTM#{3}":[86,89],"#Attention Mechanism#Tổng quan kiến trúc và pipeline cụ thể của **seq2seq with 2-layer stacked encoder + attention**#1. Encoder: 2-layer stacked LSTM#{4}":[90,92],"#Attention Mechanism#Tổng quan kiến trúc và pipeline cụ thể của **seq2seq with 2-layer stacked encoder + attention**#2. Decoder: 2-layer stacked LSTM":[93,104],"#Attention Mechanism#Tổng quan kiến trúc và pipeline cụ thể của **seq2seq with 2-layer stacked encoder + attention**#2. Decoder: 2-layer stacked LSTM#{1}":[95,95],"#Attention Mechanism#Tổng quan kiến trúc và pipeline cụ thể của **seq2seq with 2-layer stacked encoder + attention**#2. Decoder: 2-layer stacked LSTM#{2}":[96,99],"#Attention Mechanism#Tổng quan kiến trúc và pipeline cụ thể của **seq2seq with 2-layer stacked encoder + attention**#2. Decoder: 2-layer stacked LSTM#{3}":[100,104],"#Attention Mechanism#Tổng quan kiến trúc và pipeline cụ thể của **seq2seq with 2-layer stacked encoder + attention**#3. Attention Score Functions được dùng":[105,106],"#Attention Mechanism#Tổng quan kiến trúc và pipeline cụ thể của **seq2seq with 2-layer stacked encoder + attention**#Mục tiêu:":[107,124],"#Attention Mechanism#Tổng quan kiến trúc và pipeline cụ thể của **seq2seq with 2-layer stacked encoder + attention**#Mục tiêu:#{1}":[108,118],"#Attention Mechanism#Tổng quan kiến trúc và pipeline cụ thể của **seq2seq with 2-layer stacked encoder + attention**#Mục tiêu:#{2}":[119,120],"#Attention Mechanism#Tổng quan kiến trúc và pipeline cụ thể của **seq2seq with 2-layer stacked encoder + attention**#Mục tiêu:#{3}":[121,124],"#Attention Mechanism#Tổng quan kiến trúc và pipeline cụ thể của **seq2seq with 2-layer stacked encoder + attention**#4. Sinh từ tiếp theo":[125,138],"#Attention Mechanism#Tổng quan kiến trúc và pipeline cụ thể của **seq2seq with 2-layer stacked encoder + attention**#4. Sinh từ tiếp theo#{1}":[127,128],"#Attention Mechanism#Tổng quan kiến trúc và pipeline cụ thể của **seq2seq with 2-layer stacked encoder + attention**#4. Sinh từ tiếp theo#{2}":[129,131],"#Attention Mechanism#Tổng quan kiến trúc và pipeline cụ thể của **seq2seq with 2-layer stacked encoder + attention**#4. Sinh từ tiếp theo#{3}":[132,134],"#Attention Mechanism#Tổng quan kiến trúc và pipeline cụ thể của **seq2seq with 2-layer stacked encoder + attention**#4. Sinh từ tiếp theo#{4}":[135,138],"#Attention Mechanism#Tổng quan kiến trúc và pipeline cụ thể của **seq2seq with 2-layer stacked encoder + attention**#Ghi chú":[139,148],"#Attention Mechanism#Tổng quan kiến trúc và pipeline cụ thể của **seq2seq with 2-layer stacked encoder + attention**#Ghi chú#{1}":[141,141],"#Attention Mechanism#Tổng quan kiến trúc và pipeline cụ thể của **seq2seq with 2-layer stacked encoder + attention**#Ghi chú#{2}":[142,142],"#Attention Mechanism#Tổng quan kiến trúc và pipeline cụ thể của **seq2seq with 2-layer stacked encoder + attention**#Ghi chú#{3}":[143,143],"#Attention Mechanism#Tổng quan kiến trúc và pipeline cụ thể của **seq2seq with 2-layer stacked encoder + attention**#Ghi chú#{4}":[144,144],"#Attention Mechanism#Tổng quan kiến trúc và pipeline cụ thể của **seq2seq with 2-layer stacked encoder + attention**#Ghi chú#{5}":[145,145],"#Attention Mechanism#Tổng quan kiến trúc và pipeline cụ thể của **seq2seq with 2-layer stacked encoder + attention**#Ghi chú#{6}":[146,147],"#Attention Mechanism#Tổng quan kiến trúc và pipeline cụ thể của **seq2seq with 2-layer stacked encoder + attention**#Ghi chú#{7}":[148,148],"#Attention Mechanism#Image Captioning with Attention – Full Summary":[149,260],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Mục tiêu bài toán":[152,160],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Mục tiêu bài toán#{1}":[154,156],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Mục tiêu bài toán#{2}":[157,157],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Mục tiêu bài toán#{3}":[158,160],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Ý tưởng cốt lõi":[161,168],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Ý tưởng cốt lõi#{1}":[163,164],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Ý tưởng cốt lõi#{2}":[165,165],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Ý tưởng cốt lõi#{3}":[166,168],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Pipeline tổng quát":[169,193],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Pipeline tổng quát#1. Input image":[171,174],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Pipeline tổng quát#1. Input image#{1}":[173,174],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Pipeline tổng quát#2. CNN trích đặc trưng":[175,178],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Pipeline tổng quát#2. CNN trích đặc trưng#{1}":[177,177],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Pipeline tổng quát#2. CNN trích đặc trưng#{2}":[178,178],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Pipeline tổng quát#3. Attention + Decoder (RNN/LSTM)":[179,188],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Pipeline tổng quát#3. Attention + Decoder (RNN/LSTM)#{1}":[181,182],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Pipeline tổng quát#3. Attention + Decoder (RNN/LSTM)#{2}":[183,183],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Pipeline tổng quát#3. Attention + Decoder (RNN/LSTM)#{3}":[184,184],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Pipeline tổng quát#3. Attention + Decoder (RNN/LSTM)#{4}":[185,185],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Pipeline tổng quát#3. Attention + Decoder (RNN/LSTM)#{5}":[186,186],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Pipeline tổng quát#3. Attention + Decoder (RNN/LSTM)#{6}":[187,188],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Pipeline tổng quát#4. Caption generation":[189,193],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Pipeline tổng quát#4. Caption generation#{1}":[191,191],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Pipeline tổng quát#4. Caption generation#{2}":[192,193],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Công thức quan trọng":[194,201],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Công thức quan trọng#{1}":[196,196],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Công thức quan trọng#{2}":[197,197],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Công thức quan trọng#{3}":[198,198],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Công thức quan trọng#{4}":[199,201],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Attention hoạt động như thế nào?":[202,210],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Attention hoạt động như thế nào?#{1}":[204,205],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Attention hoạt động như thế nào?#{2}":[206,206],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Attention hoạt động như thế nào?#{3}":[207,207],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Attention hoạt động như thế nào?#{4}":[208,210],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Làm sao mô hình biết \"bird\" là chim?":[211,220],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Làm sao mô hình biết \"bird\" là chim?#{1}":[213,215],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Làm sao mô hình biết \"bird\" là chim?#{2}":[216,216],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Làm sao mô hình biết \"bird\" là chim?#{3}":[217,217],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Làm sao mô hình biết \"bird\" là chim?#{4}":[218,220],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Ví dụ Attention từ bài báo _Show, Attend and Tell_":[221,230],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Ví dụ Attention từ bài báo _Show, Attend and Tell_#{1}":[223,230],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Slide minh họa":[231,250],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Slide minh họa#Slide 1: Attention trên vùng ảnh":[233,239],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Slide minh họa#Slide 1: Attention trên vùng ảnh#{1}":[235,235],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Slide minh họa#Slide 1: Attention trên vùng ảnh#{2}":[236,236],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Slide minh họa#Slide 1: Attention trên vùng ảnh#{3}":[237,237],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Slide minh họa#Slide 1: Attention trên vùng ảnh#{4}":[238,239],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Slide minh họa#Slide 2: Pipeline tổng quát":[240,245],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Slide minh họa#Slide 2: Pipeline tổng quát#{1}":[242,242],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Slide minh họa#Slide 2: Pipeline tổng quát#{2}":[243,243],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Slide minh họa#Slide 2: Pipeline tổng quát#{3}":[244,245],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Slide minh họa#Slide 3: Ví dụ attention":[246,250],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Slide minh họa#Slide 3: Ví dụ attention#{1}":[248,250],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Kết luận":[251,260],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Kết luận#{1}":[253,254],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Kết luận#{2}":[255,255],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Kết luận#{3}":[256,256],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Kết luận#{4}":[257,258],"#Attention Mechanism#Image Captioning with Attention – Full Summary#Kết luận#{5}":[259,260],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)":[261,364],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Mục tiêu":[263,266],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Mục tiêu#{1}":[265,266],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Bước 1: Chuẩn bị input và tạo Query, Key, Value":[267,283],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Bước 1: Chuẩn bị input và tạo Query, Key, Value#{1}":[269,270],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Bước 1: Chuẩn bị input và tạo Query, Key, Value#{2}":[271,271],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Bước 1: Chuẩn bị input và tạo Query, Key, Value#{3}":[272,272],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Bước 1: Chuẩn bị input và tạo Query, Key, Value#{4}":[273,274],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Bước 1: Chuẩn bị input và tạo Query, Key, Value#{5}":[275,276],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Bước 1: Chuẩn bị input và tạo Query, Key, Value#Ví dụ (Input #1):":[277,283],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Bước 1: Chuẩn bị input và tạo Query, Key, Value#Ví dụ (Input #1):#{1}":[279,279],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Bước 1: Chuẩn bị input và tạo Query, Key, Value#Ví dụ (Input #1):#{2}":[280,280],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Bước 1: Chuẩn bị input và tạo Query, Key, Value#Ví dụ (Input #1):#{3}":[281,281],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Bước 1: Chuẩn bị input và tạo Query, Key, Value#Ví dụ (Input #1):#{4}":[282,283],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Bước 2: Tính Attention Scores":[284,309],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Bước 2: Tính Attention Scores#{1}":[286,289],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Bước 2: Tính Attention Scores#Ví dụ:":[290,309],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Bước 2: Tính Attention Scores#Ví dụ:#{1}":[292,295],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Bước 2: Tính Attention Scores#Ví dụ:#{2}":[296,296],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Bước 2: Tính Attention Scores#Ví dụ:#{3}":[297,297],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Bước 2: Tính Attention Scores#Ví dụ:#{4}":[298,299],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Bước 2: Tính Attention Scores#Ví dụ:#{5}":[300,309],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Bước 3: Chuẩn hóa Attention Scores bằng Softmax":[310,317],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Bước 3: Chuẩn hóa Attention Scores bằng Softmax#{1}":[312,317],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Bước 4: Tính Weighted Sum của các Value":[318,335],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Bước 4: Tính Weighted Sum của các Value#{1}":[320,321],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Bước 4: Tính Weighted Sum của các Value#Các value tương ứng:":[322,327],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Bước 4: Tính Weighted Sum của các Value#Các value tương ứng:#{1}":[324,324],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Bước 4: Tính Weighted Sum của các Value#Các value tương ứng:#{2}":[325,325],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Bước 4: Tính Weighted Sum của các Value#Các value tương ứng:#{3}":[326,327],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Bước 4: Tính Weighted Sum của các Value#Weighted sum:":[328,335],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Bước 4: Tính Weighted Sum của các Value#Weighted sum:#{1}":[330,335],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Kết quả":[336,339],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Kết quả#{1}":[338,339],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Ghi chú quan trọng":[340,345],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Ghi chú quan trọng#{1}":[342,342],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Ghi chú quan trọng#{2}":[343,343],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Ghi chú quan trọng#{3}":[344,345],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Công thức tổng quát":[346,356],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Công thức tổng quát#{1}":[348,351],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Công thức tổng quát#{2}":[352,352],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Công thức tổng quát#{3}":[353,353],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Công thức tổng quát#{4}":[354,354],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Công thức tổng quát#{5}":[355,356],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Ý nghĩa trực quan":[357,364],"#Attention Mechanism#Self-Attention Pipeline (trên từng token)#Ý nghĩa trực quan#{1}":[359,364],"#Attention Mechanism#Multi-Head Self-Attention":[365,444],"#Attention Mechanism#Multi-Head Self-Attention#Khái niệm cơ bản":[367,370],"#Attention Mechanism#Multi-Head Self-Attention#Khái niệm cơ bản#{1}":[369,370],"#Attention Mechanism#Multi-Head Self-Attention#Cấu trúc và hoạt động":[371,388],"#Attention Mechanism#Multi-Head Self-Attention#Cấu trúc và hoạt động#Input chung cho tất cả heads":[373,376],"#Attention Mechanism#Multi-Head Self-Attention#Cấu trúc và hoạt động#Input chung cho tất cả heads#{1}":[375,376],"#Attention Mechanism#Multi-Head Self-Attention#Cấu trúc và hoạt động#Điểm khác biệt quan trọng":[377,380],"#Attention Mechanism#Multi-Head Self-Attention#Cấu trúc và hoạt động#Điểm khác biệt quan trọng#{1}":[379,380],"#Attention Mechanism#Multi-Head Self-Attention#Cấu trúc và hoạt động#Công thức cho mỗi head":[381,388],"#Attention Mechanism#Multi-Head Self-Attention#Cấu trúc và hoạt động#Công thức cho mỗi head#{1}":[383,388],"#Attention Mechanism#Multi-Head Self-Attention#Ý nghĩa và lợi ích":[389,409],"#Attention Mechanism#Multi-Head Self-Attention#Ý nghĩa và lợi ích#Học nhiều góc nhìn khác nhau":[391,401],"#Attention Mechanism#Multi-Head Self-Attention#Ý nghĩa và lợi ích#Học nhiều góc nhìn khác nhau#{1}":[393,394],"#Attention Mechanism#Multi-Head Self-Attention#Ý nghĩa và lợi ích#Học nhiều góc nhìn khác nhau#{2}":[395,395],"#Attention Mechanism#Multi-Head Self-Attention#Ý nghĩa và lợi ích#Học nhiều góc nhìn khác nhau#{3}":[396,396],"#Attention Mechanism#Multi-Head Self-Attention#Ý nghĩa và lợi ích#Học nhiều góc nhìn khác nhau#{4}":[397,397],"#Attention Mechanism#Multi-Head Self-Attention#Ý nghĩa và lợi ích#Học nhiều góc nhìn khác nhau#{5}":[398,398],"#Attention Mechanism#Multi-Head Self-Attention#Ý nghĩa và lợi ích#Học nhiều góc nhìn khác nhau#{6}":[399,399],"#Attention Mechanism#Multi-Head Self-Attention#Ý nghĩa và lợi ích#Học nhiều góc nhìn khác nhau#{7}":[400,401],"#Attention Mechanism#Multi-Head Self-Attention#Ý nghĩa và lợi ích#Tăng khả năng biểu diễn":[402,409],"#Attention Mechanism#Multi-Head Self-Attention#Ý nghĩa và lợi ích#Tăng khả năng biểu diễn#{1}":[404,405],"#Attention Mechanism#Multi-Head Self-Attention#Ý nghĩa và lợi ích#Tăng khả năng biểu diễn#{2}":[406,406],"#Attention Mechanism#Multi-Head Self-Attention#Ý nghĩa và lợi ích#Tăng khả năng biểu diễn#{3}":[407,407],"#Attention Mechanism#Multi-Head Self-Attention#Ý nghĩa và lợi ích#Tăng khả năng biểu diễn#{4}":[408,409],"#Attention Mechanism#Multi-Head Self-Attention#Tổng hợp output":[410,420],"#Attention Mechanism#Multi-Head Self-Attention#Tổng hợp output#{1}":[412,417],"#Attention Mechanism#Multi-Head Self-Attention#Tổng hợp output#{2}":[418,418],"#Attention Mechanism#Multi-Head Self-Attention#Tổng hợp output#{3}":[419,420],"#Attention Mechanism#Multi-Head Self-Attention#Ví dụ minh họa":[421,444],"#Attention Mechanism#Multi-Head Self-Attention#Ví dụ minh họa#Giả sử có 3 heads cho câu \"The cat sat on the mat\"":[423,439],"#Attention Mechanism#Multi-Head Self-Attention#Ví dụ minh họa#Giả sử có 3 heads cho câu \"The cat sat on the mat\"#{1}":[425,426],"#Attention Mechanism#Multi-Head Self-Attention#Ví dụ minh họa#Giả sử có 3 heads cho câu \"The cat sat on the mat\"#{2}":[427,427],"#Attention Mechanism#Multi-Head Self-Attention#Ví dụ minh họa#Giả sử có 3 heads cho câu \"The cat sat on the mat\"#{3}":[428,429],"#Attention Mechanism#Multi-Head Self-Attention#Ví dụ minh họa#Giả sử có 3 heads cho câu \"The cat sat on the mat\"#{4}":[430,431],"#Attention Mechanism#Multi-Head Self-Attention#Ví dụ minh họa#Giả sử có 3 heads cho câu \"The cat sat on the mat\"#{5}":[432,432],"#Attention Mechanism#Multi-Head Self-Attention#Ví dụ minh họa#Giả sử có 3 heads cho câu \"The cat sat on the mat\"#{6}":[433,434],"#Attention Mechanism#Multi-Head Self-Attention#Ví dụ minh họa#Giả sử có 3 heads cho câu \"The cat sat on the mat\"#{7}":[435,436],"#Attention Mechanism#Multi-Head Self-Attention#Ví dụ minh họa#Giả sử có 3 heads cho câu \"The cat sat on the mat\"#{8}":[437,437],"#Attention Mechanism#Multi-Head Self-Attention#Ví dụ minh họa#Giả sử có 3 heads cho câu \"The cat sat on the mat\"#{9}":[438,439],"#Attention Mechanism#Multi-Head Self-Attention#Ví dụ minh họa#Kết quả cuối cùng":[440,444],"#Attention Mechanism#Multi-Head Self-Attention#Ví dụ minh họa#Kết quả cuối cùng#{1}":[442,444],"#Attention Mechanism#Transformer Architecture":[445,492],"#Attention Mechanism#Transformer Architecture#Encoder and Decoder":[446,449],"#Attention Mechanism#Transformer Architecture#Encoder and Decoder#{1}":[447,447],"#Attention Mechanism#Transformer Architecture#Encoder and Decoder#{2}":[448,449],"#Attention Mechanism#Transformer Architecture#1. Từ → Vector":[450,453],"#Attention Mechanism#Transformer Architecture#1. Từ → Vector#{1}":[451,451],"#Attention Mechanism#Transformer Architecture#1. Từ → Vector#{2}":[452,452],"#Attention Mechanism#Transformer Architecture#1. Từ → Vector#{3}":[453,453],"#Attention Mechanism#Transformer Architecture#2. Positional Encoding":[454,457],"#Attention Mechanism#Transformer Architecture#2. Positional Encoding#{1}":[456,456],"#Attention Mechanism#Transformer Architecture#2. Positional Encoding#{2}":[457,457],"#Attention Mechanism#Transformer Architecture#3. Multi-head Attention":[458,465],"#Attention Mechanism#Transformer Architecture#3. Multi-head Attention#{1}":[460,460],"#Attention Mechanism#Transformer Architecture#3. Multi-head Attention#{2}":[461,461],"#Attention Mechanism#Transformer Architecture#3. Multi-head Attention#{3}":[462,462],"#Attention Mechanism#Transformer Architecture#3. Multi-head Attention#{4}":[463,463],"#Attention Mechanism#Transformer Architecture#3. Multi-head Attention#{5}":[464,465],"#Attention Mechanism#Transformer Architecture#4. Các Mask trong Attention":[466,471],"#Attention Mechanism#Transformer Architecture#4. Các Mask trong Attention#{1}":[468,471],"#Attention Mechanism#Transformer Architecture#5. Residual Connections":[472,478],"#Attention Mechanism#Transformer Architecture#5. Residual Connections#{1}":[473,473],"#Attention Mechanism#Transformer Architecture#5. Residual Connections#{2}":[474,474],"#Attention Mechanism#Transformer Architecture#5. Residual Connections#{3}":[475,477],"#Attention Mechanism#Transformer Architecture#5. Residual Connections#{4}":[478,478],"#Attention Mechanism#Transformer Architecture#6. Feed Forward Network":[479,492],"#Attention Mechanism#Transformer Architecture#6. Feed Forward Network#{1}":[480,480],"#Attention Mechanism#Transformer Architecture#6. Feed Forward Network#{2}":[481,481],"#Attention Mechanism#Transformer Architecture#6. Feed Forward Network#Công thức:":[482,489],"#Attention Mechanism#Transformer Architecture#6. Feed Forward Network#Công thức:#{1}":[483,484],"#Attention Mechanism#Transformer Architecture#6. Feed Forward Network#Công thức:#{2}":[485,486],"#Attention Mechanism#Transformer Architecture#6. Feed Forward Network#Công thức:#{3}":[487,487],"#Attention Mechanism#Transformer Architecture#6. Feed Forward Network#Công thức:#{4}":[488,488],"#Attention Mechanism#Transformer Architecture#6. Feed Forward Network#Công thức:#{5}":[489,489],"#Attention Mechanism#Transformer Architecture#6. Feed Forward Network#Tóm lại:":[490,492],"#Attention Mechanism#Transformer Architecture#6. Feed Forward Network#Tóm lại:#{1}":[491,491],"#Attention Mechanism#Transformer Architecture#6. Feed Forward Network#Tóm lại:#{2}":[492,492],"#References":[493,494]},"outlinks":[{"title":"Pasted image 20250707004950.png","target":"Pasted image 20250707004950.png","line":464}],"task_lines":[]},